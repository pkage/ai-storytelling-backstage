#! /usr/bin/env python

import os
from textwrap import dedent
from .common import render_output_text, is_gpu_available

# this is nasty but we have to make sure the HF model cache directory is
# created before we load in the transformers library
def _setup():
    # make sure that the models cache is set up
    if not 'TRANSFORMERS_CACHE' in os.environ:
        if not os.path.exists('./model_cache/hf'):
            os.makedirs('./model_cache/hf')

        # set the environment variable
        os.environ['TRANSFORMERS_CACHE'] = os.path.abspath('./model_cache/hf')

_setup()


from transformers import pipeline, set_seed # type: ignore
from transformers import AutoModelForSeq2SeqLM, AutoTokenizer
from transformers.utils.logging import set_verbosity_error

from typing import List, Any, Optional
from IPython.display import Markdown, display


# silence errors from deep within Transformers
set_verbosity_error()


def _seed_if_necessary(seed: Optional[int]):
    '''
    Quick function that sets the seed if it's passed. If not provided,
    no seed will be explicitly set

    :param seed: Seed to set or None or -1
    '''
    if seed is not None and seed != -1:
        set_seed(seed)


def _get_pipeline_device(accelerate: bool = True):
    '''
    Determine CUDA device to use.

    :param accelerate: (optional) Whether to use acceleration if it is available. Default True
    :return: The pipeline device argument
    '''
    if accelerate and is_gpu_available():
        return 0

    return -1


def summarization(
        text: str,
        model: str ='facebook/bart-large-cnn',
        max_length: int =130,
        min_length: int =30,
        do_sample: bool = False,
        accelerate: bool = True,
        seed: Optional[int] = None,
        render: bool = True
    ):
    '''
    Summarize text from a prompt.

    :param text: The text to summarize.
    :param model: (optional) The model to use for summarization (default `facebook/bart-large-cnn`)
    :param max_length: (optional) The minimum length of the summary. (default 30)
    :param min_length: (optional) The maximum length of the summary. (default 130)
    :param do_sample: (optional) Whether to subsample input (default False)
    :param accelerate: (optional) Whether to use GPU acceleration (if available). Default True
    :param seed: (optional) Seed value for reproducible pipeline runs.
    :param render: (optional) Automatically render results for an ipython notebook 
                   if one is detected. Default True
    :return: A summarization of the original text.
    '''
    _seed_if_necessary(seed)

    device = _get_pipeline_device(accelerate=accelerate)
    model_name = model # for 

    tokenizer = AutoTokenizer.from_pretrained(model_name).to(device)
    model = AutoModelForSeq2SeqLM.from_pretrained(model_name).to(device)
    
    # 2. Preprocess: Tokenize the input text
    # The pipeline handles both single strings and lists of strings
    inputs = tokenizer(
        text, 
        return_tensors="pt", 
        padding=True, 
        truncation=True
    ).to(device)
    
    # 3. Forward: Generate output IDs
    # Incorporates the default generation config mentioned in the class
    summary_ids = model.generate(
        inputs["input_ids"],
        attention_mask=inputs.get("attention_mask"),
        max_new_tokens=1000,
        num_beams=8,
    )
    
    # 4. Postprocess: Decode the tokens back to text
    results = []
    for output_id in summary_ids:
        decoded_text = tokenizer.decode(
            output_id, 
            skip_special_tokens=True, 
            clean_up_tokenization_spaces=False
        )
        results.append(decoded_text)
        
    # skip render attempt if not requested
    if not render:
        return results[0]

    return render_output_text(results[0])


def text_generation(
        prompt: str, 
        max_length: int = 200,
        num_return_sequences: int = 3,
        model: str ='small',
        seed: Optional[int] =None,
        accelerate: bool = True,
        render: bool = True
    ):
    '''
    Generate text from a prompt.

    Options for model are:
        - 'small', 'medium', 'large' (mapping to distilgpt2, gpt2, and gpt-2-large)
        - any other text model on HF

    Some models to try:
        - EleutherAI/gpt-j-6b
        - facebook/opt-1.3b

    :param prompt: Text to prompt the pipeline with.
    :param model: (optional) Model to use. Default 'small'.
    :param max_length: (optional) Length of text to generate. Default 200.
    :param num_return_sequences: (optional) Number of different responses to make. Default 3.
    :param accelerate: (optional) Whether to use GPU acceleration (if available). Default True
    :param seed: (optional) Seed value for reproducible pipeline runs.
    :param render: (optional) Automatically render results for an ipython notebook 
                   if one is detected. Default True

    :returns: A list of text generated by the model.
    '''

    _seed_if_necessary(seed)

    if model in ['small', 'medium', 'large']:
        mapping = {
            'small': 'distilgpt2',
            'medium': 'gpt2',
            'large': 'gpt2-large'
        }
        model = mapping[model]


    device = _get_pipeline_device(accelerate=accelerate)
    pipe = pipeline(task='text-generation', model=model, device=device)

    results = pipe(
        prompt,
        max_length=max_length,
        num_return_sequences=num_return_sequences
    )

    # convert results to a list of strings
    results = [r['generated_text'] for r in results]

    # skip render attempt if not requested
    if not render:
        return results

    return render_output_text(results)



def sentiment_analysis(
        text: str,
        model: str = 'distilbert-base-uncased-finetuned-sst-2-english',
        accelerate: bool = True,
        seed: Optional[int] = None,
        render: bool =True
    ):
    '''
    Analyze the sentiment of a particular piece of text.

    :param text: The text to analyze.
    :param model: (optional) The model to use for analysis. (default 'distilbert-base-uncased-finetuned-sst-2-english')
    :param accelerate: (optional) Whether to use GPU acceleration (if available). Default True
    :param seed: (optional) Seed value for reproducible pipeline runs.
    :param render: (optional) Automatically render results for an ipython notebook 
                   if one is detected. Default True
    :return: The most likely sentiment of the text.
    '''
    _seed_if_necessary(seed)

    device = _get_pipeline_device(accelerate=accelerate)
    pipe = pipeline(task='sentiment-analysis', model=model, device=device)

    sentiment = pipe(text)[0]

    if not render:
        return sentiment

    color = 'red' if sentiment['label'] == 'NEGATIVE' else 'green';
    return display(Markdown(f'<span style="color: {color};">{sentiment["label"]}</span> (score: {sentiment["score"]*100}%)'))


def mask_filling(
        text: str,
        model: str = 'bert-base-uncased',
        accelerate: bool = True,
        seed: Optional[int] = None,
        render: bool = True
    ):
    '''
    Guess words that fill a specific slot in some text. The default mask token is [MASK].

    :param text: The text to fill, with the mask token in it.
    :param model: (optional) The model to use. (default 'bert-base-uncased')
    :param accelerate: (optional) Whether to use GPU acceleration (if available). Default True
    :param seed: (optional) Seed value for reproducible pipeline runs.
    :param render: (optional) Automatically render results for an ipython notebook 
                   if one is detected. Default True
    :return: A string with the mask filled in.
    '''
    _seed_if_necessary(seed)

    device = _get_pipeline_device(accelerate=accelerate)
    pipe = pipeline(task='fill-mask', model=model, device=device)

    masks = pipe(text)

    if not render:
        return masks

    # helper to render a string as bold
    def render_mask_result(i, obj):
        bolded = obj['sequence'].replace(obj['token_str'], f'**<span style="color: blue;">{obj["token_str"]}</span>**')

        return f'Sample {i+i} ({100*obj["score"]:4}\n\n> {bolded}'

    # render each item with a seqeunce number
    masks = '\n\n'.join([render_mask_result(i, obj) for i, obj in enumerate(masks)])

    return display(Markdown(masks))


def question_answering(
        question: str,
        context: str,
        model: str = 'deepset/roberta-base-squad2',
        accelerate: bool = True,
        seed: Optional[int] = None,
        render: bool =True
    ):
    '''
    Answer a question about some given context.

    :param question: The question to answer from the data.
    :param context: The context from which to draw the answer.
    :param model: (optional) The model to use. (default 'deepset/roberta-base-squad2')
    :param accelerate: (optional) Whether to use GPU acceleration (if available). Default True
    :param seed: (optional) Seed value for reproducible pipeline runs.
    :param render: (optional) Automatically render results for an ipython notebook 
                   if one is detected. Default True
    :return: 
    :rtype: 
    '''
    _seed_if_necessary(seed)

    device = _get_pipeline_device(accelerate=accelerate)
    pipe = pipeline(task='question-answering', model=model, device=device)

    answer = pipe({
        'question': question,
        'context': context
    })

    if not render:
        return answer

    return display(Markdown(dedent(f'''\
        Answer: **{answer['answer']}**

        Score: `{answer['score']}`  
        Position: {answer['start']} to {answer['end']}
    ''')))


def instruct(
        prompt: str,
        model: str = 'mistralai/Mistral-7B-Instruct-v0.2',
        accelerate: bool = True,
        seed: Optional[int] = None,
        render: bool =True
    ):
    '''
    Have an instruction-tuned model respond to a prompt.

    :param text: The text to fill, with the mask token in it.
    :param model: (optional) The model to use. (default 'mistralai/Mistral-7B-Instruct-v0.2')
    :param accelerate: (optional) Whether to use GPU acceleration (if available). Default True
    :param seed: (optional) Seed value for reproducible pipeline runs.
    :param render: (optional) Automatically render results for an ipython notebook 
                   if one is detected. Default True
    :return: A response from the model.
    '''
    # putting this in here just in case it's not supported
    from transformers import AutoModelForCausalLM, AutoTokenizer

    _seed_if_necessary(seed)

    device = _get_pipeline_device(accelerate=accelerate)

    model = AutoModelForCausalLM.from_pretrained('mistralai/Mistral-7B-Instruct-v0.2')
    tokenizer = AutoTokenizer.from_pretrained('mistralai/Mistral-7B-Instruct-v0.2')

    messages = [
        {'role': 'user', 'content': prompt}
    ]

    encodeds = tokenizer.apply_chat_template(messages, return_tensors="pt")

    model_inputs = encodeds.to(device)
    model.to(device)

    generated_ids = model.generate(model_inputs, max_new_tokens=1000, do_sample=True)
    decoded = tokenizer.batch_decode(generated_ids)
    answer = decoded[0]

    if not render:
        return answer

    return display(Markdown(dedent(f'''\
        {answer}
    ''')))
